databricks-job-monitoring

ğŸ“Š Monitoramento de Performance de Jobs no Databricks

ğŸ“Œ DescriÃ§Ã£o do Projeto
Este projeto tem como objetivo coletar logs de execuÃ§Ã£o dos jobs no Databricks, processÃ¡-los e armazenÃ¡-los para futuras anÃ¡lises. A ideia Ã© criar um pipeline de monitoramento que permita entender o desempenho dos jobs e detectar falhas ou lentidÃ£o.

ğŸš€ Tecnologias Utilizadas
Python â†’ Para extrair e processar os logs da API do Databricks.
Databricks API â†’ Para obter informaÃ§Ãµes dos jobs executados.
Pandas â†’ Para manipulaÃ§Ã£o dos dados extraÃ­dos.
Parquet â†’ Para armazenamento otimizado dos logs.

ğŸ“‚ Estrutura do Projeto
ğŸ“ monitoramento-jobs-databricks
â”‚â€” ğŸ“„ databricks_logs_extraction.py  # Script para coletar logs da API
â”‚â€” ğŸ“„ README.md  # DocumentaÃ§Ã£o do projeto

ğŸ”§ Como Executar o Script

ğŸ“Œ Etapa AC1 â€“ ExtraÃ§Ã£o de Dados
Configurar as credenciais do Databricks

No arquivo databricks_logs_extraction.py, substitua:

DATABRICKS_URL = "https://<seu-workspace>.cloud.databricks.com"
DATABRICKS_TOKEN = "<seu-token-aqui>"

Coloque o Databricks Workspace URL e o Token de AutenticaÃ§Ã£o.

Instalar as dependÃªncias (caso necessÃ¡rio):

pip install requests pandas pyarrow

Rodar o script:

python databricks_logs_extraction.py

Verificar a saÃ­da:
O script gerarÃ¡ um arquivo databricks_logs.parquet contendo os logs extraÃ­dos.

ğŸ“Œ Etapa AC2 â€“ Processamento e EstruturaÃ§Ã£o dos Dados (06/04)

Nesta etapa foi criado um pipeline em Python (Google Colab) para processar os logs brutos extraÃ­dos da API do Databricks. O objetivo foi estruturar os dados para facilitar anÃ¡lises futuras e gerar um novo arquivo tratado.

ğŸ”§ Etapas realizadas:

Leitura do arquivo databricks_logs.parquet (gerado na AC1)

ConversÃ£o dos timestamps para datetime

CÃ¡lculo da duraÃ§Ã£o de execuÃ§Ã£o dos jobs (duration_minutes)

CriaÃ§Ã£o da coluna data_execucao

SeleÃ§Ã£o de colunas relevantes (job_id, run_id, state, result_state, etc.)

GeraÃ§Ã£o do novo arquivo logs_processados.parquet

AnÃ¡lise exploratÃ³ria simples (mÃ©dia de duraÃ§Ã£o por resultado, quantidade de execuÃ§Ãµes por dia)

ğŸ“ Arquivos gerados:

logs_processados.parquet

ğŸ› ï¸ Ferramentas:

Python (Google Colab)

Pandas

PyArrow (Parquet)

ğŸ“Œ PrÃ³ximos Passos (AC3 e Entrega Final)
AC3 (04/05): Criar dashboard e sistema de alertas para falhas.
Entrega Final (08/06): Refinar o cÃ³digo e documentaÃ§Ã£o final do projeto.

ğŸ“Œ Links Importantes
RepositÃ³rio no GitHub
Board do GitHub Projects

